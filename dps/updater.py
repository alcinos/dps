import abc
from future.utils import with_metaclass

import tensorflow as tf

from dps import cfg
from dps.utils import (
    add_scaled_noise_to_gradients,
    adj_inverse_time_decay, build_scheduled_value, build_optimizer)


class Param(object):
    pass


class Updater(with_metaclass(abc.ABCMeta, object)):

    def __init__(self, env, **kwargs):
        self.env = env
        self._n_experiences = 0
        self._resolve_params(kwargs)

        self.build_graph()

    def _resolve_params(self, kwargs):
        for p in self.params:
            value = kwargs.get(p)
            if value is None:
                value = getattr(cfg, p)
            setattr(self, p, value)

    @property
    def params(self):
        return [p for p in dir(self) if p != 'params' and isinstance(getattr(self, p), Param)]

    @property
    def stage(self):
        return 0

    @property
    def n_experiences(self):
        return self._n_experiences

    def update(self, batch_size, summary_op=None):
        self._n_experiences += batch_size
        return self._update(batch_size, summary_op)

    @abc.abstractmethod
    def _update(self, batch_size, summary_op=None):
        raise Exception()

    def build_graph(self):
        self._build_graph()

    @abc.abstractmethod
    def _build_graph(self):
        raise Exception()

    def _build_train(self):
        """ Add ops to implement training. ``self.loss`` must already be defined. """

        tf.summary.scalar('loss', self.loss)

        lr = build_scheduled_value(self.lr_schedule, 'learning_rate')

        self.optimizer = build_optimizer(self.optimizer_spec, lr)

        tvars = tf.trainable_variables()
        self.pure_gradients = tf.gradients(self.loss, tvars)
        tf.summary.scalar('gradient_norm_pure', tf.global_norm(self.pure_gradients))

        if hasattr(self, 'max_grad_norm') and self.max_grad_norm is not None and self.max_grad_norm > 0.0:
            self.clipped_gradients, _ = tf.clip_by_global_norm(self.pure_gradients, self.max_grad_norm)
        else:
            self.clipped_gradients = self.pure_gradients

        tf.summary.scalar('gradient_norm_clipped', tf.global_norm(self.clipped_gradients))

        global_step = tf.contrib.framework.get_or_create_global_step()

        if hasattr(self, 'noise_schedule') and self.noise_schedule is not None:
            grads_and_vars = zip(self.clipped_gradients, tvars)
            start, decay_steps, decay_rate, staircase = self.noise_schedule
            noise = adj_inverse_time_decay(
                start, global_step, decay_steps, decay_rate, staircase=staircase, gamma=0.55)
            tf.summary.scalar('gradient_noise_amount', noise)
            self.noisy_gradients = add_scaled_noise_to_gradients(grads_and_vars, noise)
        else:
            self.noisy_gradients = self.clipped_gradients
        tf.summary.scalar('gradient_norm_clipped_and_noisy', tf.global_norm(self.noisy_gradients))

        grads_and_vars = list(zip(self.noisy_gradients, tvars))
        self.train_op = self.optimizer.apply_gradients(grads_and_vars, global_step=global_step)

        # if cfg.debug:
        #     for grad, var in self.gradients:
        #         tf.histogram_summary(var.name, var)

    def save(self, path, step=None):
        g = tf.get_default_graph()
        tvars = g.get_collection('trainable_variables')
        saver = tf.train.Saver(tvars)
        return saver.save(tf.get_default_session(), path, step)

    def restore(self, path):
        g = tf.get_default_graph()
        tvars = g.get_collection('trainable_variables')
        saver = tf.train.Saver(tvars)
        saver.restore(tf.get_default_session(), path)


class ReinforcementLearningUpdater(Updater):
    """ Update parameters of a policy using reinforcement learning.

    Should be used in the context of both a default session, default graph and default context.

    Parameters
    ----------
    env: gym Env
        The environment we're trying to learn about.
    policy: callable object
        Needs to provide member functions ``build_feeddict`` and ``get_output``.

    """
    optimizer_spec = Param()
    lr_schedule = Param()
    noise_schedule = Param()
    max_grad_norm = Param()
    gamma = Param()
    l2_norm_penalty = Param()

    def __init__(self,
                 env,
                 policy,
                 **kwargs):

        self.policy = policy

        self.obs_dim = env.observation_space.shape[1]
        self.n_actions = env.action_space.shape[1]

        super(ReinforcementLearningUpdater, self).__init__(env, **kwargs)

    def start_episode(self):
        pass

    def end_episode(self):
        pass

    def remember(self, obs, action, reward, behaviour_policy=None):
        """ Supply the RL algorithm with a unit of experience.

        If behaviour_policy==None, assumes that data was generated by self.policy.

        """
        if not self.policy:
            raise ValueError("Policy has not been set using ``set_policy``.")
